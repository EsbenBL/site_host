<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Methodology on Network Communities of the Social Sciences</title>
    <link>https://esbenbl.github.io/site_host/docs/methodology/</link>
    <description>Recent content in Methodology on Network Communities of the Social Sciences</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://esbenbl.github.io/site_host/docs/methodology/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>hSBM</title>
      <link>https://esbenbl.github.io/site_host/docs/methodology/hsbm_meth/hsbm_meth/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esbenbl.github.io/site_host/docs/methodology/hsbm_meth/hsbm_meth/</guid>
      <description>Hierachical Stochastic Block Model#hSBM briefly explained The model is used to identify prevalent topics in a text corpora, and it relies on integration of techniques used for community detection in networks (Gerlach et al. 2018). Another option for us could have been latent Dirichlet allocation (LDA), but hSBM has proven to perform better than the more widely used LDA (ibid). Topics can be identified by the hSBM by representing the text corpora as a bipartite network in which nodes are documents and words, and the weight of the edges connecting nodes are determined by word-occurrences in each document.</description>
    </item>
    
    <item>
      <title>Louvain Modularity</title>
      <link>https://esbenbl.github.io/site_host/docs/methodology/Modularity/modularity/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esbenbl.github.io/site_host/docs/methodology/Modularity/modularity/</guid>
      <description>Louvain Modularity#The Louvain algorithm partitions the network by finding the partition that optimizes modularity, defined as (Barab√°si eq. 9.12): \(M = \sum_{c=1}^{n_c}\left[\frac{L_c}{L}-\left(\frac{k_c}{2L}\right)^2\right]\), where \(n_c\)is the number of communities/partitions, \(L\)is the total number of links in the network, \(L_c\)is the total number of links within community \(C_c\), and \(k_c\)is the total degree of the nodes in this community (including links to nodes outside community \(C_c\)).</description>
    </item>
    
    <item>
      <title>TF and TF-IDF</title>
      <link>https://esbenbl.github.io/site_host/docs/methodology/tf_idf_method/tf_idf/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://esbenbl.github.io/site_host/docs/methodology/tf_idf_method/tf_idf/</guid>
      <description>TF and TF-IDF#According to Wiki, there is a variaty of ways to calculate TF-IDF. We calculate TF-IDF in what appears to be the most conventional way, where the raw count, \( f_{t,d}\), of term \(t\)in document \( d\)is standardized/adjusted by the document length, \( \sum_{t&amp;#39;\in{d}}f_{t&amp;#39;,d}\), e.i. adjusted by the raw count of all the terms, \( t&amp;#39;\), in \(d\).</description>
    </item>
    
  </channel>
</rss>
